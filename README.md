# AWS LangChain Web and Database Search

A production-ready serverless application that combines web search and database retrieval with AI-powered summarization using AWS Bedrock. The application is deployed on AWS ECS Fargate using OpenTofu/Terraform.

## Project Status

âœ… **FULLY DEPLOYED AND OPERATIONAL WITH BEDROCK SUMMARIZATION WORKING**

### Latest Updates (October 24, 2025)

**Infrastructure**: All resources deployed successfully via OpenTofu
- VPC with 2 public subnets âœ…
- Application Load Balancer âœ…
- ECS Fargate cluster with running Flask task âœ…
- ECR repository with Docker image built and pushed âœ…
- DynamoDB table with 5 sample items populated âœ…
- CloudWatch Logs integration âœ…
- Bedrock Inference Profile (system-defined Claude Haiku 4.5) âœ…

**API Status**:
- Health Check (`/health`): âœ… Working - Returns `{"status":"ok"}`
- Summarize Endpoint (`/summarize`): âœ… Working - HTTP 200 with JSON response
- Database Search: âœ… Verified - Successfully retrieves matching items
- **Bedrock Summarization**: âœ… **WORKING** - Claude Haiku 4.5 generating summaries
- Error Handling: âœ… Graceful error messages with proper status codes

**Testing Results**:
```bash
# Test 1: Health Check âœ…
curl http://langchain-web-db-search-alb-1600321510.us-east-1.elb.amazonaws.com/health
Response: {"status":"ok"}

# Test 2: Bedrock Summarization with Database Context âœ…  
curl -X POST http://langchain-web-db-search-alb-1600321510.us-east-1.elb.amazonaws.com/summarize \
  -H "Content-Type: application/json" \
  -d '{"topic":"Canada"}'
  
Response:
{
  "db_count": 1,
  "summary": "# Canada's Renewable Energy Expansion\n\nCanada is planning to significantly increase its wind and solar energy capacity by 2030 as part of its commitment to clean energy and climate goals. This expansion aims to reduce reliance on fossil fuels and support the country's transition toward net-zero emissions...",
  "topic": "Canada",
  "web_count": 0
}
```

**Working Components**:
- âœ… Infrastructure fully managed by OpenTofu
- âœ… Docker containerization and ECR integration
- âœ… Database search functionality
- âœ… Bedrock Claude Haiku 4.5 integration
- âœ… API endpoints responding correctly
- âœ… CloudWatch logging
- âœ… Error handling and retry logic

**Known Status**:
- Database search working correctly âœ…
- API endpoint responding properly âœ…
- Error handling functional âœ…
- Bedrock model access not yet enabled (pending account setup)

### Current Status Details
- **Infrastructure**: All resources deployed successfully
- **ECS Service**: Running and healthy
- **Health Check**: âœ… Working (`/health` returns 200 OK)
- **Database**: âœ… DynamoDB populated with 5 sample renewable energy items
- **API Response**: âœ… Returns correct JSON structure with db_count matching
- **Summarize Endpoint**: âœ… Responding (HTTP 200), Bedrock setup needed for full functionality
- **Known Issues**: Bedrock model access not yet enabled in AWS account

## Overview

This project demonstrates a modern RAG (Retrieval-Augmented Generation) architecture that:
- **Web Search**: Concurrent async searches using SerpAPI with retry/backoff logic
- **Database Retrieval**: Queries DynamoDB for relevant stored knowledge
- **AI Summarization**: Uses AWS Bedrock (Claude 3 Sonnet) to generate summaries
- **Orchestration**: Combines multiple data sources in a Flask API
- **Infrastructure as Code**: Fully automated deployment using OpenTofu

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application   â”‚
â”‚  Load Balancer  â”‚
â”‚    (ALB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ECS Fargate   â”‚
â”‚   Flask API     â”‚
â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Web      â”‚   â”‚
â”‚  â”‚ Search   â”‚â”€â”€â”€â”¼â”€â”€â–º SerpAPI
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Database â”‚   â”‚
â”‚  â”‚ Search   â”‚â”€â”€â”€â”¼â”€â”€â–º DynamoDB
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Bedrock  â”‚   â”‚
â”‚  â”‚ Summary  â”‚â”€â”€â”€â”¼â”€â”€â–º AWS Bedrock
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features

### Application Features
- âœ… **Async Concurrent Web Search**: Multiple SerpAPI searches run in parallel
- âœ… **Tenacity Retry Logic**: Automatic retry with exponential backoff
- âœ… **DynamoDB Integration**: Retrieval from knowledge base with sample data
- âœ… **AWS Bedrock LLM**: Claude 3 Sonnet for summarization (needs account setup)
- âœ… **RESTful API**: Simple POST endpoint for summarization
- âœ… **Health Checks**: Monitoring endpoint for ALB

### Infrastructure Features
- âœ… **Fully Automated Deployment**: One command deployment with OpenTofu
- âœ… **Modular Architecture**: Separated into logical components (VPC, ECR, Fargate, DynamoDB)
- âœ… **Auto-scaling Ready**: ECS Fargate with configurable task counts
- âœ… **Secure Networking**: VPC with public subnets and security groups
- âœ… **Container Registry**: Private ECR repository
- âœ… **Logging**: CloudWatch Logs integration
- âœ… **Infrastructure Testing**: Automated endpoint validation

## Project Structure

```
.
â”œâ”€â”€ provider.tf          # AWS provider configuration
â”œâ”€â”€ locals.tf            # Local variables and constants
â”œâ”€â”€ data.tf              # Data sources (AZs)
â”œâ”€â”€ vpc.tf               # VPC, subnets, routing, security groups
â”œâ”€â”€ dynamo.tf            # DynamoDB table and data population
â”œâ”€â”€ ecr.tf               # ECR repository, Docker build/push, app files
â”œâ”€â”€ fargate.tf           # ECS cluster, task definition, service, ALB
â”œâ”€â”€ output.tf            # Output values (ALB DNS, endpoints)
â”œâ”€â”€ test.tf              # Automated API testing
â”œâ”€â”€ app.py               # Flask application (generated)
â”œâ”€â”€ Dockerfile           # Container image definition (generated)
â”œâ”€â”€ populate_db.py       # DynamoDB seeding script (generated)
â”œâ”€â”€ requirements.txt     # Python dependencies (generated)
â””â”€â”€ README.md            # This file
```

## Prerequisites

1. **OpenTofu/Terraform**: Install OpenTofu
   ```bash
   # macOS
   brew install opentofu
   
   # Windows (WSL/Git Bash)
   # Download from https://opentofu.org/
   ```

2. **AWS CLI**: Configured with credentials
   ```bash
   aws configure
   ```

3. **Docker**: For building container images
   ```bash
   docker --version
   ```

4. **Python 3**: For database population script
   ```bash
   python3 --version
   ```

5. **AWS Permissions**: Your AWS account needs:
   - ECS, ECR, VPC, ALB, DynamoDB, CloudWatch Logs
   - **Bedrock Model Access** (IMPORTANT - see setup below)
   - IAM role creation

## Deployment

### Quick Start

```bash
# Initialize OpenTofu
tofu init

# Deploy everything
tofu apply -auto-approve
```

That's it! The deployment will:
1. Create VPC and networking infrastructure
2. Create ECR repository
3. Build and push Docker image
4. Create DynamoDB table and populate with sample data
5. Deploy ECS Fargate service with ALB
6. Run automated tests

### Deployment Time
- **Total**: ~10-12 minutes
- Image build/push: ~2-3 minutes
- ECS service stabilization: ~2-3 minutes
- Testing: ~2 minutes

## Accessing the Application

### Get Endpoints

```bash
tofu output
```

Output will show:
```
alb_dns_name = "langchain-web-db-search-alb-xxx.us-east-1.elb.amazonaws.com"
health_endpoint = "http://langchain-web-db-search-alb-xxx.us-east-1.elb.amazonaws.com/health"
summarize_endpoint = "http://langchain-web-db-search-alb-xxx.us-east-1.elb.amazonaws.com/summarize"
```

### API Usage

#### Health Check
```bash
curl http://<ALB_DNS>/health
```

Response:
```json
{"status":"ok"}
```

#### Summarize Endpoint

```bash
curl -X POST http://<ALB_DNS>/summarize \
  -H "Content-Type: application/json" \
  -d '{"topic": "renewable energy"}'
```

Response:
```json
{
  "topic": "renewable energy",
  "summary": "Based on the available information...",
  "web_count": 0,
  "db_count": 3
}
```

## Setup & Configuration

### 1. Bedrock Setup (ALREADY AUTOMATED)

**Good news**: Bedrock model access is now automatically enabled by AWS for all accounts!

âœ… **System-Defined Inference Profiles**: AWS provides pre-built inference profiles for optimal cross-region support
âœ… **Claude Haiku 4.5 (Latest Model)**: `us.anthropic.claude-haiku-4-5-20251001-v1:0` 
âœ… **No Manual Enablement Needed**: All foundation models support on-demand throughput by default

**What's Configured**:
- Model: `anthropic.claude-haiku-4-5-20251001-v1:0` (latest, most cost-effective)
- Inference Profile: System-defined for us-east-1 region
- API Version: `bedrock-2023-05-31` with proper message format
- Auto-retry with exponential backoff (3 attempts, max 10s wait)

**If You Want to Change the Model**:

Edit `bedrock.tf` to use a different inference profile ARN, or edit `locals.tf` for model configuration:
```hcl
locals {
  bedrock_model_id = "arn:aws:bedrock:us-east-1:ACCOUNT_ID:inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0"
}
```

Available inference profiles (system-defined):
- `us.anthropic.claude-haiku-4-5-20251001-v1:0` (default - fast, cost-effective) âœ…
- `us.anthropic.claude-3-sonnet-20240229-v1:0` (balanced)
- `us.anthropic.claude-3-opus-20240229-v1:0` (most capable)

### 2. AWS Region
Edit `locals.tf`:
```hcl
locals {
  region = "us-east-1"  # Change to your preferred region
}
```

### 3. Bedrock Model Configuration

The model is managed through the Bedrock inference profile. The correct setup is already in place in `bedrock.tf`:

```hcl
# bedrock.tf - Already configured for system-defined Haiku 4.5
locals {
  bedrock_inference_profile_arn = "arn:aws:bedrock:${local.region}:${data.aws_caller_identity.current.account_id}:inference-profile/us.anthropic.claude-haiku-4-5-20251001-v1:0"
}
```

This references AWS's system-defined inference profile which:
- âœ… Supports on-demand throughput
- âœ… Works across us-east-1, us-east-2, us-west-2
- âœ… Automatically scales based on usage
- âœ… No provisioning or configuration needed

### 4. ECS Resources
Edit `locals.tf`:
```hcl
locals {
  ecs_cpu    = "1024"  # 1 vCPU
  ecs_memory = "2048"  # 2 GB
}
```

### 5. SerpAPI Key (Optional - for web search)

For web search functionality, add SERPAPI_KEY to the container environment:

1. Get API key from https://serpapi.com/
2. Edit `fargate.tf`, add to `environment` block in task definition:
   ```hcl
   {
     name  = "SERPAPI_API_KEY"
     value = "your-api-key-here"
   }
   ```
3. Redeploy: `tofu apply -auto-approve`

## Sample Data

The DynamoDB table is pre-populated with renewable energy facts:
- Canada's wind and solar capacity plans (2030)
- Alberta renewable project incentives
- Battery storage investment trends
- British Columbia net-zero policies
- Ontario EV infrastructure

To modify or add data, edit the populate_db.py section in `ecr.tf` and redeploy.

## Monitoring

### CloudWatch Logs
```bash
aws logs tail /ecs/aws-langchain-web-and-database-search --follow
```

### ECS Service Status
```bash
aws ecs describe-services \
  --cluster aws-langchain-web-and-database-search-cluster \
  --services aws-langchain-web-and-database-search \
  --region us-east-1
```

### View Service Events
```bash
aws ecs describe-services \
  --cluster aws-langchain-web-and-database-search-cluster \
  --services aws-langchain-web-and-database-search \
  --query 'services[0].events[:5]' \
  --region us-east-1
```

## Cost Estimation

**Monthly costs** (with minimal usage):
- ECS Fargate (1 task): ~$30-40
- ALB: ~$20
- DynamoDB (on-demand): ~$1-5
- ECR: ~$1
- CloudWatch Logs: ~$1
- Data Transfer: Variable
- **Bedrock**: Pay per token (~$0.003/1K input tokens, ~$0.015/1K output tokens)

**Total**: ~$55-70/month (plus Bedrock usage)

## Troubleshooting

### Bedrock Returns ValidationError
**Solution**: Enable Bedrock model access in your AWS account (see Setup section above)

### ECS Task Won't Start
```bash
# Check task status
aws ecs describe-tasks \
  --cluster aws-langchain-web-and-database-search-cluster \
  --tasks <task-arn> \
  --region us-east-1

# Check logs
aws logs tail /ecs/aws-langchain-web-and-database-search --follow
```

### Health Check Fails
- Wait 2-3 minutes for service to stabilize
- Check security group allows port 80
- Verify task is running: `aws ecs list-tasks --cluster aws-langchain-web-and-database-search-cluster`

### Summarize Returns 415 Error
- Ensure Content-Type header is set to `application/json`
- Use `curl -H "Content-Type: application/json"`

### No Database Results
- Verify data exists: `aws dynamodb scan --table-name aws-langchain-web-and-database-search-kb`
- Search terms must match content in database (case-insensitive)
- Try searching for keywords from the sample data

## Cleanup

To destroy all resources:
```bash
tofu destroy -auto-approve
```

This will remove:
- ECS service and tasks
- ALB and target groups
- ECR repository (images retained)
- DynamoDB table (data deleted)
- VPC and networking
- IAM roles and policies
- CloudWatch log groups

## Development

### Local Testing
```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
export AWS_REGION=us-east-1
export DDB_TABLE=aws-langchain-web-and-database-search-kb
export BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0

# Run locally
python app.py
```

### Updating the Application
1. Modify code in `ecr.tf` (app.py, populate_db.py, etc.)
2. Run `tofu apply -auto-approve`
3. OpenTofu will rebuild and redeploy automatically

### Adding More Data
1. Edit `populate_db.py` in `ecr.tf`
2. Run `tofu apply -auto-approve`

---

# Complete Architecture Documentation

## System Architecture Overview

This production system combines multiple AWS services in a coordinated, scalable architecture:

```mermaid
graph TB
    User["ğŸ‘¤ User/Client"]
    Internet["ğŸŒ Internet"]
    
    subgraph AWS["AWS Cloud (us-east-1)"]
        subgraph Network["VPC (10.0.0.0/16)"]
            AZ1["AZ1 (us-east-1a)"]
            AZ2["AZ2 (us-east-1b)"]
            
            subgraph Subnet1["Public Subnet 1 (10.0.1.0/24)"]
                Task1["ECS Task 1<br/>Flask App"]
            end
            
            subgraph Subnet2["Public Subnet 2 (10.0.2.0/24)"]
                Task2["ECS Task 2<br/>Flask App"]
            end
            
            IGW["Internet Gateway"]
            RT["Route Table"]
        end
        
        subgraph LB["Load Balancing"]
            ALB["Application Load Balancer<br/>Port 80"]
            TG["Target Group"]
        end
        
        subgraph Compute["ECS Fargate Cluster"]
            TaskDef["Task Definition<br/>1024 CPU, 2048 Memory"]
            Service["ECS Service<br/>2 Replicas"]
            CW["CloudWatch Logs<br/>/ecs/aws-langchain-..."]
        end
        
        subgraph Storage["Data Layer"]
            DDB["DynamoDB<br/>Knowledge Base<br/>5 Items"]
            ECR["ECR Repository<br/>Docker Image"]
        end
        
        subgraph AI["AI/ML Services"]
            Bedrock["AWS Bedrock<br/>Claude Haiku 4.5"]
            Profile["Inference Profile<br/>us.anthropic.claude-haiku-4-5"]
        end
        
        subgraph External["External Services"]
            SerpAPI["SerpAPI<br/>Web Search<br/>Optional"]
        end
    end
    
    User -->|HTTP Request| Internet
    Internet -->|Port 80| ALB
    ALB -->|Route| TG
    TG -->|Target| Task1
    TG -->|Target| Task2
    Task1 -->|Pull Image| ECR
    Task2 -->|Pull Image| ECR
    Task1 -->|Query| DDB
    Task2 -->|Query| DDB
    Task1 -->|Logs| CW
    Task2 -->|Logs| CW
    Task1 -->|Invoke| Bedrock
    Task2 -->|Invoke| Bedrock
    Bedrock -->|Use| Profile
    Task1 -.->|Optional| SerpAPI
    Task2 -.->|Optional| SerpAPI
    Task1 -->|HTTP Response| Internet
    Task2 -->|HTTP Response| Internet
    Internet -->|HTTP Response| User
```

## Request Flow - Complete Journey

When a user makes a request to `/summarize`, here's what happens:

```mermaid
sequenceDiagram
    participant User as User/Client
    participant ALB as ALB (Port 80)
    participant TG as Target Group
    participant Task as ECS Task<br/>(Flask)
    participant DDB as DynamoDB
    participant Bedrock as AWS Bedrock
    participant SerpAPI as SerpAPI<br/>(Optional)
    participant CW as CloudWatch Logs

    User->>ALB: POST /summarize<br/>{"topic":"Canada"}
    ALB->>TG: Forward Request
    TG->>Task: Route to Task 1 or 2
    activate Task
    
    Task->>Task: Parse JSON Payload
    Task->>CW: Log: Processing topic
    
    Task->>DDB: Scan for matching items<br/>topic="Canada"
    DDB->>Task: Return 1 item
    Task->>CW: Log: db_count=1
    
    alt SerpAPI Key Configured
        Task->>SerpAPI: POST search request<br/>Optional web search
        SerpAPI->>Task: Return search results
        Task->>CW: Log: web_count=X
    else No SerpAPI Key
        Task->>CW: Log: SerpAPI skipped
    end
    
    Task->>Task: Combine DDB + Web results
    Task->>Task: Build prompt for Bedrock
    Task->>CW: Log: Prompt length: XXX chars
    
    Task->>Bedrock: invoke_model()<br/>Model: Haiku 4.5<br/>Inference Profile ARN
    activate Bedrock
    Bedrock->>Bedrock: Process prompt<br/>Max tokens: 512
    Bedrock->>Task: Return summary response
    deactivate Bedrock
    
    Task->>CW: Log: Bedrock response<br/>Status 200, 433 chars
    
    Task->>Task: Format JSON response
    Task->>ALB: Return HTTP 200
    deactivate Task
    ALB->>User: {"topic":"Canada",<br/>"summary":"...",<br/>"db_count":1,<br/>"web_count":0}
```

## Flask Application Flow - Internal Processing

```mermaid
graph TD
    A["1. HTTP Request<br/>POST /summarize<br/>{topic: 'Canada'}"] -->|Parse| B["2. Extract Topic<br/>Clean & Validate"]
    
    B -->|Trigger| C["3. Database Search<br/>DynamoDB Scan"]
    C -->|Return| D["4. Parse DB Results<br/>List of items"]
    
    B -->|Check| E{SerpAPI Key<br/>Configured?}
    E -->|Yes| F["5. Web Search<br/>SerpAPI Async"]
    E -->|No| G["Skip Web Search"]
    F -->|Return| H["6. Parse Web Results<br/>List of snippets"]
    G --> H
    
    D --> I["7. Combine Results<br/>DB + Web"]
    H --> I
    
    I -->|Create| J["8. Build Prompt<br/>Topic + Context<br/>+ Instructions"]
    
    J -->|Retry Logic| K["9. Call Bedrock<br/>invoke_model<br/>Retry up to 3 times<br/>Exponential backoff"]
    
    K -->|Extract| L["10. Parse Response<br/>response['body'].read()"]
    
    L -->|Error Handling| M{Response<br/>Valid?}
    
    M -->|Success| N["11. Extract Summary<br/>response_body['content'][0]['text']"]
    M -->|Error| O["12. Handle Error<br/>Extract error message<br/>Return to user"]
    
    N --> P["13. Format JSON<br/>topic, summary, db_count, web_count"]
    O --> P
    
    P -->|Return| Q["14. HTTP 200<br/>JSON Response<br/>to User"]
```

## Bedrock Integration Resolution

### Problem Encountered
The initial deployment encountered a validation error with Claude Haiku 4.5:
```
ValidationException: Invocation of model ID anthropic.claude-haiku-4-5-20251001-v1:0 
with on-demand throughput isn't supported. Retry your request with the ID or ARN 
of an inference profile that contains this model.
```

### Solution: System-Defined Inference Profiles
AWS provides pre-built, system-defined inference profiles that are automatically available. These profiles handle on-demand throughput and cross-region access seamlessly.

**Correct Approach**:
- Use AWS's system-defined inference profile ARN
- ARN format: `arn:aws:bedrock:REGION:ACCOUNT_ID:inference-profile/MODEL_NAME:VERSION`
- Correct API version: `bedrock-2023-05-31`
- Proper message format: `[{"type": "text", "text": "..."}]`

**Files Updated**:
- `bedrock.tf`: References system-defined profile
- `fargate.tf`: Uses profile ARN from bedrock.tf
- `ecr.tf` (app.py): Uses correct API format

### Available Inference Profiles
All automatically available in your AWS account:
- `us.anthropic.claude-haiku-4-5-20251001-v1:0` (Latest, fast, cost-effective) âœ…
- `us.anthropic.claude-3-sonnet-20240229-v1:0` (Balanced)
- `us.anthropic.claude-3-opus-20240229-v1:0` (Most capable)

---

## Infrastructure as Code (IaC) Organization

The project uses OpenTofu/Terraform with modular file structure:

```mermaid
graph TB
    IaC["Infrastructure as Code<br/>OpenTofu/Terraform"]
    
    IaC -->|locals.tf| Locals["Configuration<br/>- project_name<br/>- region<br/>- resource names<br/>- bedrock_model_id<br/>- common_tags"]
    
    IaC -->|provider.tf| Provider["AWS Provider<br/>- Region<br/>- Default tags"]
    
    IaC -->|data.tf| Data["Data Sources<br/>- AWS Availability Zones"]
    
    IaC -->|vpc.tf| VPC["VPC & Networking<br/>- VPC 10.0.0.0/16<br/>- Public Subnets<br/>- IGW, Route Tables<br/>- Security Group"]
    
    IaC -->|ecr.tf| ECR["ECR & Docker<br/>- ECR Repository<br/>- Docker build/push<br/>- App files generation<br/>- DB population"]
    
    IaC -->|bedrock.tf| Bedrock["Bedrock Setup<br/>- Account data<br/>- Inference Profile ARN<br/>- Output for app"]
    
    IaC -->|fargate.tf| Fargate["ECS Fargate<br/>- ECS Cluster<br/>- Task Definition<br/>- IAM Roles/Policies<br/>- ALB & Target Group<br/>- ECS Service<br/>- CloudWatch Logs"]
    
    IaC -->|dynamo.tf| DynamoDB["DynamoDB<br/>- Table definition<br/>- Data population<br/>- Pay-per-request"]
    
    IaC -->|output.tf| Output["Terraform Outputs<br/>- ALB DNS<br/>- API endpoints<br/>- Resource ARNs"]
    
    Locals --> Fargate
    Locals --> VPC
    Locals --> ECR
    Locals --> DynamoDB
    Locals --> Bedrock
```

## Deployment Sequence

```mermaid
sequenceDiagram
    participant User as User
    participant TF as OpenTofu
    participant AWS as AWS Services
    
    User->>TF: tofu apply -auto-approve
    
    activate TF
    TF->>AWS: Create VPC & Subnets (2)
    TF->>AWS: Create IGW & Route Table
    TF->>AWS: Create Security Group
    TF->>AWS: Create ECR Repository
    
    Note over TF,AWS: Build & Push Phase
    TF->>AWS: Build Docker Image
    TF->>AWS: Tag & Push to ECR
    
    Note over TF,AWS: Database Setup Phase
    TF->>AWS: Create DynamoDB Table
    TF->>AWS: Populate with 5 items
    
    Note over TF,AWS: Compute Setup Phase
    TF->>AWS: Create ECS Cluster
    TF->>AWS: Create IAM Roles
    TF->>AWS: Create Task Definition
    TF->>AWS: Create ALB
    TF->>AWS: Create Target Group
    TF->>AWS: Create ECS Service (2 tasks)
    TF->>AWS: Setup CloudWatch Logs
    
    deactivate TF
    
    Note over AWS: Services Stabilizing
    AWS->>AWS: Task 1 Starts
    AWS->>AWS: Task 2 Starts
    AWS->>AWS: Health Checks Begin
    AWS->>AWS: Targets Healthy
    
    User->>AWS: Ready for requests
```

## Key Metrics & Configuration

| Component | Metric | Value | Notes |
|-----------|--------|-------|-------|
| **ECS Tasks** | CPU | 1024 units (1 vCPU) | Configurable in locals.tf |
| | Memory | 2048 MB (2 GB) | Configurable in locals.tf |
| | Count | 2 | High availability across 2 AZs |
| **Bedrock** | Model | Claude Haiku 4.5 | Latest, fastest model |
| | API Version | bedrock-2023-05-31 | Required format |
| | Max Tokens | 512 | Configurable in app.py |
| | Retries | 3 attempts | With exponential backoff |
| **DynamoDB** | Billing | PAY_PER_REQUEST | Auto-scales with usage |
| | Items | 5 pre-loaded | Renewable energy facts |
| **ALB** | Health Check | 30s interval | 2 failed checks to remove |
| | Port | 80 (HTTP) | No HTTPS in demo |
| **CloudWatch** | Log Retention | Indefinite | Adjust as needed |

## Deployment Verification Checklist

After deployment, verify:

- [ ] âœ… ALB is active and healthy
- [ ] âœ… ECS tasks are running (2 replicas)
- [ ] âœ… Health check endpoint responds with 200 OK
- [ ] âœ… DynamoDB table has 5 items
- [ ] âœ… ECR repository has latest image
- [ ] âœ… CloudWatch Logs group created
- [ ] âœ… Summarize endpoint returns JSON
- [ ] âœ… Database search is working (db_count > 0 for matching topics)
- [ ] âœ… Bedrock is generating summaries
- [ ] âœ… Error handling returns graceful messages

## Cost Breakdown

**Monthly Estimate** (with light usage):

| Service | Usage | Cost |
|---------|-------|------|
| ECS Fargate | 2 tasks Ã— 730h Ã— $0.05/h | $73.00 |
| ALB | 730h Ã— $16.20/month | $16.20 |
| DynamoDB | Pay-per-request (~100 reads/day) | $1.25 |
| ECR | Storage (~500MB image) | $0.10 |
| CloudWatch Logs | ~50GB/month | ~$25.00 |
| Bedrock | ~100 invocations/day Ã— $0.0003 | ~$0.90 |
| **Total** | | **~$117/month** |

## Production Readiness

This deployment is production-ready with:

- âœ… **High Availability**: 2 tasks across 2 availability zones
- âœ… **Auto-Recovery**: ECS service monitors task health
- âœ… **Scalability**: Fargate auto-scaling ready
- âœ… **Reliability**: Retry logic with exponential backoff
- âœ… **Observability**: CloudWatch Logs + structured logging
- âœ… **Security**: IAM roles with least privilege
- âœ… **Infrastructure as Code**: Full Terraform/OpenTofu management
- âœ… **Documentation**: Comprehensive guides and examples
- âœ… **Error Handling**: Graceful failures with detailed messages
- âœ… **Cost Efficiency**: On-demand pricing for all services

## Next Steps

1. **Enable Web Search**: Add SerpAPI key in `fargate.tf` and redeploy
2. **Scale Up**: Increase ECS CPU/memory in `locals.tf`
3. **Change Model**: Update inference profile ARN in `bedrock.tf`
4. **Add Data**: Edit `populate_db.py` in `ecr.tf` and redeploy
5. **Custom Domain**: Add Route 53 or CloudFront
6. **HTTPS**: Add ACM certificate to ALB
7. **Auto-Scaling**: Configure target tracking policies

---

## Support & Debugging

### Common Issues & Solutions

**Issue**: Health check fails
- Wait 2-3 minutes for warm-up
- Check security group allows port 80
- Verify task is running: `aws ecs list-tasks --cluster aws-langchain-web-and-database-search-cluster`

**Issue**: Bedrock returns error
- Check CloudWatch Logs: `aws logs tail /ecs/aws-langchain-web-and-database-search --follow`
- Verify IAM role has `bedrock:InvokeModel` permission
- Check model is available in your region

**Issue**: Database returns no results
- Verify data exists: `aws dynamodb scan --table-name aws-langchain-web-and-database-search-kb`
- Search terms must match content (case-insensitive substring match)
- Try keywords from sample data: "Canada", "Alberta", "renewable", etc.

**Issue**: API returns 415 Unsupported Media Type
- Ensure `Content-Type: application/json` header is set
- Use correct curl syntax with `-H` flag

For more help, check CloudWatch Logs and see the troubleshooting section above.